# parse_xml.py
import xml.etree.ElementTree as ET
import re
import os
import json
import codecs
from bs4 import BeautifulSoup
import pandas as pd

df_codes = pd.read_csv(
    "./company_overview_codes.csv",
    dtype={"corp_code": str, "induty_code": str}
)


def preprocess_xml_content(xml_string):
    # í—ˆìš©ë˜ëŠ” íƒœê·¸ ì´ë¦„ ë¦¬ìŠ¤íŠ¸
    WHITELIST = {
        "DOCUMENT", "DOCUMENT-NAME", "FORMULA-VERSION", "COMPANY-NAME", "SUMMARY",
        "LIBRARY", "BODY", "EXTRACTION", "COVER", "COVER-TITLE",
        "IMAGE", "IMG", "IMG-CAPTION",  "P", "A", "SPAN",
        "TR", "TD", "TH", "TE", "TU", "SECTION-1", "SECTION-2", "SECTION-3",
        "TITLE", "TABLE", "TABLE-GROUP", "COLGROUP", "COL", "THEAD",
        "TBODY", "PGBRK", "PART", "CORRECTION"
    }
    # TAG_RE = re.compile(
    #     r"<(/?)([A-Za-z0-9\-]+)"               # ê·¸ë£¹ 1: ìŠ¬ë˜ì‹œ?, ê·¸ë£¹ 2: íƒœê·¸ëª…
    #     r"([^>]*)"                             # ê·¸ë£¹ 3: ì†ì„± ë“±
    #     r">"
    # )
    # íƒœê·¸ ì´ë¦„ë§Œ ì¶”ì¶œí•˜ëŠ” ì •ê·œì‹: &lt;/TD&gt; -> /TD, &lt;TD ...&gt; -> TD ...
    TAG_RE = re.compile(r"&lt;(/?)(\w+(?:-\w+)*)([^&]*)&gt;")
    def restore_whitelisted_tags(match):
        slash, tag, attrs = match.groups()
        if tag.upper() in WHITELIST:
            return f"<{slash}{tag}{attrs}>"
        return match.group(0) # í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ì— ì—†ìœ¼ë©´ ê·¸ëŒ€ë¡œ ë‘ 
    
    def repl(m):
        slash, tag, attrs = m.group(1), m.group(2), m.group(3)
        if tag not in WHITELIST:
            # ì „ì²´ íƒœê·¸ ì—”í‹°í‹° ì²˜ë¦¬
            inner = m.group(0)[1:-1]  # "<...>" ì‚¬ì´ ë¬¸ìì—´
            return "&lt;" + inner + "&gt;"
        else:
            # í—ˆìš©ëœ íƒœê·¸: ì†ì„±ì€ ê·¸ëŒ€ë¡œ ìœ ì§€
            return f"<{slash}{tag}{attrs}>"

    # 1. XML ì„ ì–¸ (Processing Instruction) ì¶”ì¶œ ë° ë³´í˜¸
    xml_declaration_match = re.match(r"<\?xml[^>]*\?>\s*", xml_string)
    xml_declaration = ""
    remaining_xml_string = xml_string

    if xml_declaration_match:
        xml_declaration = xml_declaration_match.group(
            0
        )  # ë§¤ì¹˜ëœ ì„ ì–¸ê³¼ ë’¤ë”°ë¥´ëŠ” ê³µë°± í¬í•¨
        remaining_xml_string = xml_string[
            xml_declaration_match.end() :
        ]  # ì„ ì–¸ ë¶€ë¶„ ì œê±°

    encoded_xml = remaining_xml_string.replace("<", "&lt;").replace(">", "&gt;")
    cleaned_xml = TAG_RE.sub(restore_whitelisted_tags, encoded_xml)

    # SPAN(ìŠ¤íƒ€ì¼), A(ë§í¬) íƒœê·¸ ì œê±°
    span_pattern = re.compile(r'</?SPAN\b[^>]*?>', re.IGNORECASE)
    cleaned_xml_string = span_pattern.sub('', cleaned_xml)
    a_pattern = re.compile(r'</?A\b[^>]*?>', re.IGNORECASE)
    cleaned_xml_string = a_pattern.sub('', cleaned_xml_string)

    # 2. ì „ì²´ ë¬¸ìì—´ ì•ë’¤ì˜ ê³µë°± ë° ê°œí–‰ ë¬¸ì ì œê±° (XML ì„ ì–¸ ì œì™¸í•œ ë‚˜ë¨¸ì§€ ë¶€ë¶„ì— ì ìš©)
    cleaned_xml_string = cleaned_xml_string.strip()

    # 3. XML 1.0 ì‚¬ì–‘ì—ì„œ í—ˆìš©ë˜ì§€ ì•ŠëŠ” ì œì–´ ë¬¸ì ì œê±° (ParseError ë°©ì§€)
    cleaned_xml_string = re.sub(
        r"[\x00-\x08\x0b\x0c\x0e-\x1f\x7f]", "", cleaned_xml_string
    )

    # 4. ì´ìŠ¤ì¼€ì´í”„ë˜ì§€ ì•Šì€ '&' ë¬¸ìë¥¼ '&amp;'ë¡œ ë³€í™˜
    cleaned_xml_string = re.sub(r'&(?!#|amp;)', r'&amp;', cleaned_xml_string)

    if xml_declaration:
        # ì›ë˜ ì„ ì–¸ ë’¤ì— ê³µë°±ì´ë‚˜ ê°œí–‰ì´ ìˆì—ˆë‹¤ë©´ ê·¸ê²ƒì„ ìœ ì§€
        # ìƒˆë¡œìš´ ì‹œì‘ ë¬¸ìì—´ ì•ì—ë„ ê°œí–‰ì„ ë„£ì–´ ì¤„ ë§ì¶¤ì„ ì‹œë„í•©ë‹ˆë‹¤.
        cleaned_xml_string = xml_declaration + "\n" + cleaned_xml_string.lstrip()
    # print(cleaned_xml_string)

    return cleaned_xml_string


def clean_table_html_for_llm(html_string):
    """
    LLMì— ì „ë‹¬í•˜ê¸° ìœ„í•œ í…Œì´ë¸” HTMLì—ì„œ ë¶ˆí•„ìš”í•œ ë ˆì´ì•„ì›ƒ/ìŠ¤íƒ€ì¼ ì†ì„±ì„ ì œê±°í•˜ê³ 
    ë¹„í‘œì¤€ íƒœê·¸ (TE, TU)ë¥¼ í‘œì¤€ TDë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    ROWSPAN, COLSPANê³¼ ê°™ì€ êµ¬ì¡° ê´€ë ¨ ì†ì„±ì€ ìœ ì§€í•©ë‹ˆë‹¤.
    ë˜í•œ, HTML ë¬¸ìì—´ ë‚´ì˜ ë¶ˆí•„ìš”í•œ ì¤„ë°”ê¿ˆê³¼ ì—­ìŠ¬ë˜ì‹œë¥¼ ì œê±°í•©ë‹ˆë‹¤.
    """
    soup = BeautifulSoup(html_string, "html.parser")

    # ë¶ˆí•„ìš”í•œ ì†ì„± ì œê±° ëª©ë¡ (ì†Œë¬¸ìë¡œ ë¹„êµ)
    # LLMì—ê²ŒëŠ” ë¶ˆí•„ìš”í•˜ê±°ë‚˜ ì‹œê°ì  ì •ë³´ì¸ ì†ì„±ë“¤
    attrs_to_remove = [
        "width",
        "height",
        "align",
        "valign",
        "aclass",
        "afixtable",
        "acopy",
        "adelete",
        "aupdatecont",
        "acopycol",
        "amovecol",
        "adeletecol",
        "usermark",
        "acode",
        "aunit",
        "aunitvalue",
        "refno",
        "aassocnote",
        "atoc",
        "atocid",
        "adelim",  # DART íŠ¹ìœ ì˜ ë©”íƒ€ë°ì´í„° ì†ì„±ë“¤
        "border",
        "frame",
        "rules",  # ìš”ì²­ì— ë”°ë¼ ì¶”ê°€
        "style",
        "class",
        "id",  # ì¼ë°˜ì ìœ¼ë¡œ HTMLì—ì„œ ë¶ˆí•„ìš”í•œ ì†ì„± ì¶”ê°€
    ]

    for tag in soup.find_all(True):  # ëª¨ë“  íƒœê·¸ ìˆœíšŒ (ìì‹  í¬í•¨)
        # ë¹„í‘œì¤€ íƒœê·¸ (TE, TU)ë¥¼ í‘œì¤€ TDë¡œ ë³€í™˜
        if tag.name == "te" or tag.name == "tu":
            tag.name = "td"

        # ë¶ˆí•„ìš”í•œ ì†ì„± ì œê±°
        for attr_name in list(tag.attrs.keys()):
            if attr_name.lower() in attrs_to_remove:
                del tag.attrs[attr_name]

    cleaned_html = str(soup)
    # ê³ ê°ë‹˜ì˜ ì œì•ˆëŒ€ë¡œ ìˆœì„œëŒ€ë¡œ ì²˜ë¦¬: ì¤„ë°”ê¿ˆ ì œê±° -> ì—­ìŠ¬ë˜ì‹œ ì œê±° -> ì—°ì† ê³µë°± ì••ì¶•
    cleaned_html = cleaned_html.replace("\n", "")  # ëª¨ë“  ì¤„ë°”ê¿ˆ ì œê±°
    cleaned_html = cleaned_html.replace('\\', '')  # ëª¨ë“  ì—­ìŠ¬ë˜ì‹œ ì œê±°

    return cleaned_html.strip()  # ìµœì¢…ì ìœ¼ë¡œ ì•ë’¤ ê³µë°± ì œê±°


def extract_content_recursive(element, collected_items):
    """
    ì£¼ì–´ì§„ Elementì™€ ê·¸ í•˜ìœ„ Elementë“¤ì„ ì¬ê·€ì ìœ¼ë¡œ íƒìƒ‰í•˜ì—¬
    BORDER="1"ì¸ TABLEì€ HTMLë¡œ (ì •ì œ í›„), ê·¸ ì™¸ì˜ í…ìŠ¤íŠ¸ëŠ” ì¼ë°˜ í…ìŠ¤íŠ¸ë¡œ collected_itemsì— ì¶”ê°€í•©ë‹ˆë‹¤.
    """
    # BORDER="1"ì¸ TABLEì„ ë°œê²¬í•˜ë©´ HTMLë¡œ ì¶”ì¶œí•˜ê³  ì´ ê°€ì§€ì˜ íƒìƒ‰ì€ ì¤‘ë‹¨
    if element.tag == "TABLE" and element.get("BORDER") == "1":
        table_html = ET.tostring(element, encoding="utf-8").decode("utf-8").strip()
        cleaned_table_html = clean_table_html_for_llm(
            table_html
        )  # LLMì„ ìœ„í•´ HTML ì •ì œ
        collected_items.append(
            {"type": "table", "content": cleaned_table_html}
        )  # íƒ€ì… ì¶”ê°€
        return

    # í˜„ì¬ ì—˜ë¦¬ë¨¼íŠ¸ì˜ ì§ì ‘ì ì¸ í…ìŠ¤íŠ¸ ë…¸ë“œ ì²˜ë¦¬
    if element.text and element.text.strip():
        # í…ìŠ¤íŠ¸ ë‚´ë¶€ì˜ ì—¬ëŸ¬ ê³µë°±(ê°œí–‰ í¬í•¨)ì„ ë‹¨ì¼ ê³µë°±ìœ¼ë¡œ ì •ê·œí™”
        normalized_text = re.sub(r"\s+", " ", element.text.strip())
        if normalized_text:  # ë¹„ì–´ìˆì§€ ì•Šì€ ê²½ìš°ì—ë§Œ ì¶”ê°€
            collected_items.append({"type": "text", "content": normalized_text})

    # ìì‹ ì—˜ë¦¬ë¨¼íŠ¸ë“¤ì„ ì¬ê·€ì ìœ¼ë¡œ íƒìƒ‰
    for child in element:
        extract_content_recursive(child, collected_items)

        # ìì‹ ì—˜ë¦¬ë¨¼íŠ¸ì˜ tail í…ìŠ¤íŠ¸ ë…¸ë“œ ì²˜ë¦¬
        if child.tail and child.tail.strip():
            # tail í…ìŠ¤íŠ¸ ë‚´ë¶€ì˜ ì—¬ëŸ¬ ê³µë°±(ê°œí–‰ í¬í•¨)ì„ ë‹¨ì¼ ê³µë°±ìœ¼ë¡œ ì •ê·œí™”
            normalized_tail_text = re.sub(r"\s+", " ", child.tail.strip())
            if normalized_tail_text:  # ë¹„ì–´ìˆì§€ ì•Šì€ ê²½ìš°ì—ë§Œ ì¶”ê°€
                collected_items.append(
                    {"type": "text", "content": normalized_tail_text}
                )

def parse_darter_xml(xml_content, file_name):
    """
    DART ê³µì‹œë³´ê³ ì„œ XML ë‚´ìš©ì„ íŒŒì‹±í•˜ê³  ì£¼ìš” ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    SECTION-1ê³¼ SECTION-2ë¥¼ ì¤‘ì²© ë°˜ë³µë¬¸ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    """
    processed_xml_content = preprocess_xml_content(xml_content)

    try:
        root = ET.fromstring(processed_xml_content)
    except ET.ParseError as e:
        # ì˜¤ë¥˜ ì²˜ë¦¬ ë¡œì§ì€ ê¸°ì¡´ê³¼ ë™ì¼
        print(f"XML íŒŒì‹± ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None
    

    # ìŠ¤í‚µ ì¡°ê±´: <TITLE> ì¤‘ í…ìŠ¤íŠ¸ê°€ "ì¦ê¶Œë°œí–‰ì¡°ê±´í™•ì •"ì¸ ê²½ìš°
    skip_title = root.find(".//TITLE")
    if skip_title is not None and skip_title.text and skip_title.text.strip() == "ì¦ê¶Œë°œí–‰ì¡°ê±´í™•ì •":
        return None   # ğŸš¨ ì´ ë¬¸ì„œëŠ” ì²˜ë¦¬í•˜ì§€ ì•ŠìŒ


    # ìµœìƒìœ„ ë ˆë²¨ ë°ì´í„° ì¶”ì¶œ (ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼)
    doc_id = file_name.split(".")[0]
    pub_date = file_name[:8]

    # ... (doc_name, doc_code, corp_code, corp_name ì¶”ì¶œ ë¡œì§) ...
    doc_name_element = root.find("DOCUMENT-NAME")
    doc_name = doc_name_element.text.strip() if doc_name_element is not None else ""
    doc_code = doc_name_element.get("ACODE") if doc_name_element is not None else ""
    company_name_element = root.find("COMPANY-NAME")
    corp_code = (
        company_name_element.get("AREGCIK") if company_name_element is not None else ""
    )
    corp_name = (
        company_name_element.text.strip() if company_name_element is not None else ""
    )
    induty_code = get_induty_code(corp_code)
    # ...

    report_data = {
        "doc_id": doc_id,
        "doc_name": doc_name,
        "doc_code": doc_code,
        "pub_date": pub_date,
        "corp_code": corp_code,
        "corp_name": corp_name,
        "induty_code": induty_code,
        "sections": [],
    }

    sec_id_counter = 0

    # 1. SECTION-1 ì—˜ë¦¬ë¨¼íŠ¸ë¥¼ ì°¾ì•„ì„œ ìˆœíšŒ
    section1_elements = root.findall(".//SECTION-1")

    for section1_element in section1_elements:
        # SECTION-1ì˜ ì œëª©ì„ ì¶”ì¶œ
        title1_element = section1_element.find("TITLE")

        if title1_element is not None and title1_element.text and title1_element.text.strip():
            sec_id_counter += 1
            section1_data = {
                "sec_id": f"{sec_id_counter}",
                "sec_title": title1_element.text.strip(),
                "sec_content": "",
            }

            collected_items_for_section1 = []
            
            # SECTION-1ì˜ ìì‹ë“¤ì„ ìˆœíšŒ (SECTION-2 ì œì™¸)
            for child_of_section1 in section1_element:
                if child_of_section1.tag == "SECTION-2":
                    # SECTION-2ëŠ” ë³„ë„ë¡œ ì²˜ë¦¬í•˜ë¯€ë¡œ ê±´ë„ˆë›°ê¸°
                    continue
                if child_of_section1.tag == "TITLE":
                    continue
                extract_content_recursive(child_of_section1, collected_items_for_section1)
            
            # SECTION-1ì˜ ì½˜í…ì¸ ë¥¼ í•©ì¹˜ê³  ì €ì¥
            section1_data["sec_content"] = _combine_contents(collected_items_for_section1)
            report_data["sections"].append(section1_data)

        # 2. SECTION-1 ì•„ë˜ì— ìˆëŠ” SECTION-2 ì—˜ë¦¬ë¨¼íŠ¸ë¥¼ ì°¾ì•„ì„œ ìˆœíšŒ
        section2_elements = section1_element.findall("./SECTION-2")

        for section2_element in section2_elements:
            # SECTION-2ì˜ ì œëª©ì„ ì¶”ì¶œ
            title2_element = section2_element.find("TITLE")

            if title2_element is not None and title2_element.text and title2_element.text.strip():
                sec_id_counter += 1
                section2_data = {
                    "sec_id": f"{sec_id_counter}",
                    "sec_title": title2_element.text.strip(),
                    "sec_content": "",
                }

                collected_items_for_section2 = []
                
                # SECTION-2ì˜ ìì‹ë“¤ì„ ìˆœíšŒ (SECTION-3ë¶€í„°ëŠ” ëª¨ë‘ ì½˜í…ì¸ )
                for child_of_section2 in section2_element:
                    if child_of_section2.tag == "TITLE":
                        continue
                    extract_content_recursive(child_of_section2, collected_items_for_section2)
                
                # SECTION-2ì˜ ì½˜í…ì¸ ë¥¼ í•©ì¹˜ê³  ì €ì¥
                section2_data["sec_content"] = _combine_contents(collected_items_for_section2)
                report_data["sections"].append(section2_data)

    return report_data

def _combine_contents(items):
    """
    ì½˜í…ì¸  ì•„ì´í…œ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°›ì•„ì„œ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ í•©ì¹˜ëŠ” í—¬í¼ í•¨ìˆ˜
    (ê¸°ì¡´ ì½”ë“œì˜ ì½˜í…ì¸  í•©ì¹˜ëŠ” ë¡œì§ì„ ë³„ë„ í•¨ìˆ˜ë¡œ ë¶„ë¦¬)
    """
    final_sec_content_parts_builder = []
    prev_type = None

    for item in items:
        current_content = item["content"]
        current_type = item["type"]
        if final_sec_content_parts_builder:
            if prev_type != current_type or (
                current_type == "text" and not current_content.strip()
            ):
                if not final_sec_content_parts_builder[-1].endswith("\n"):
                    final_sec_content_parts_builder.append("\n")
        
        final_sec_content_parts_builder.append(current_content)
        prev_type = current_type

    raw_final_content = "".join(final_sec_content_parts_builder).strip()
    final_sec_content = re.sub(r"\s+", " ", raw_final_content)
    final_sec_content = re.sub(r"\n+", "\n", final_sec_content)
    final_sec_content = final_sec_content.strip()
    return final_sec_content

# if __name__ == "__main__":
#     # íŒŒì‹±ì˜¤ë¥˜ ë°œìƒ XML íŒŒì¼ë“¤
#     # XML_FILE_PATH = "20240430000817.xml"
#     # XML_FILE_PATH = "20240516000056.xml"
#     # XML_FILE_PATH = "20240514001094.xml"
#     # XML_FILE_PATH = "20240514001108.xml"
#     # XML_FILE_PATH = "20240524000535.xml"
#     XML_FILE_PATH = "20240110000519.xml"
#     with codecs.open(XML_FILE_PATH, "r", encoding="utf-8") as f:
#         xml_content = f.read()
#     parsed_data = parse_darter_xml(xml_content, os.path.basename(XML_FILE_PATH))
#     if parsed_data:
#         print(json.dumps(parsed_data, ensure_ascii=False, indent=2))
#     else:
#         print("Parsing failed.")




def get_induty_code(corp_code: str) -> str:
    """corp_codeë¥¼ ì…ë ¥ë°›ì•„ induty_code ë°˜í™˜"""
    row = df_codes[df_codes['corp_code'] == corp_code]
    if not row.empty:
        return row.iloc[0]['induty_code']
    return None  # ì—†ìœ¼ë©´ None ë°˜í™˜